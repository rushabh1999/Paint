#!/usr/bin/env python3
"""
Query and summarize Internet Uptime Tracker logs.

This script reads the CSV log files generated by the uptime tracker
and provides statistics and summaries.
"""

import csv
import sys
import os
from datetime import datetime, timedelta
from pathlib import Path
import argparse


def parse_log_file(log_file):
    """Parse a CSV log file and return entries."""
    entries = []
    
    try:
        with open(log_file, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                entry = {
                    'timestamp': datetime.strptime(row['Timestamp'], '%Y-%m-%d %H:%M:%S'),
                    'status': row['Status'],
                    'latency_ms': int(row['Latency_ms']) if row['Latency_ms'] != '-1' else None,
                    'outage_duration_sec': int(row['Outage_Duration_sec'])
                }
                entries.append(entry)
    except Exception as e:
        print(f"Error reading log file: {e}", file=sys.stderr)
        return None
    
    return entries


def calculate_statistics(entries):
    """Calculate uptime/downtime statistics from log entries."""
    if not entries:
        return None
    
    stats = {
        'total_checks': len(entries),
        'successful_checks': 0,
        'failed_checks': 0,
        'uptime_percentage': 0.0,
        'downtime_percentage': 0.0,
        'total_outages': 0,
        'total_downtime_sec': 0,
        'longest_outage_sec': 0,
        'shortest_outage_sec': float('inf'),
        'avg_outage_sec': 0.0,
        'avg_latency_ms': 0.0,
        'min_latency_ms': float('inf'),
        'max_latency_ms': 0,
        'first_check': entries[0]['timestamp'],
        'last_check': entries[-1]['timestamp']
    }
    
    latencies = []
    
    for entry in entries:
        if entry['status'] == 'UP':
            stats['successful_checks'] += 1
            if entry['latency_ms'] is not None:
                latencies.append(entry['latency_ms'])
            
            # Check if this marks the end of an outage
            if entry['outage_duration_sec'] > 0:
                stats['total_outages'] += 1
                stats['total_downtime_sec'] += entry['outage_duration_sec']
                stats['longest_outage_sec'] = max(stats['longest_outage_sec'], entry['outage_duration_sec'])
                stats['shortest_outage_sec'] = min(stats['shortest_outage_sec'], entry['outage_duration_sec'])
        else:
            stats['failed_checks'] += 1
    
    # Calculate percentages
    if stats['total_checks'] > 0:
        stats['uptime_percentage'] = (stats['successful_checks'] / stats['total_checks']) * 100
        stats['downtime_percentage'] = (stats['failed_checks'] / stats['total_checks']) * 100
    
    # Calculate average outage duration
    if stats['total_outages'] > 0:
        stats['avg_outage_sec'] = stats['total_downtime_sec'] / stats['total_outages']
    
    # Calculate latency statistics
    if latencies:
        stats['avg_latency_ms'] = sum(latencies) / len(latencies)
        stats['min_latency_ms'] = min(latencies)
        stats['max_latency_ms'] = max(latencies)
    else:
        stats['min_latency_ms'] = 0
    
    # Fix shortest outage if no outages
    if stats['shortest_outage_sec'] == float('inf'):
        stats['shortest_outage_sec'] = 0
    
    return stats


def format_duration(seconds):
    """Format duration in seconds to human-readable format."""
    if seconds < 60:
        return f"{seconds} seconds"
    elif seconds < 3600:
        minutes = seconds / 60
        return f"{minutes:.2f} minutes"
    else:
        hours = seconds / 3600
        return f"{hours:.2f} hours"


def print_statistics(stats):
    """Print formatted statistics."""
    if not stats:
        print("No statistics available.")
        return
    
    print("\n" + "="*60)
    print("Internet Uptime Statistics")
    print("="*60)
    
    print(f"\nPeriod: {stats['first_check']} to {stats['last_check']}")
    duration = (stats['last_check'] - stats['first_check']).total_seconds()
    print(f"Total Duration: {format_duration(duration)}")
    
    print(f"\n--- Connection Status ---")
    print(f"Total Checks: {stats['total_checks']}")
    print(f"Successful Checks: {stats['successful_checks']}")
    print(f"Failed Checks: {stats['failed_checks']}")
    print(f"Uptime Percentage: {stats['uptime_percentage']:.2f}%")
    print(f"Downtime Percentage: {stats['downtime_percentage']:.2f}%")
    
    if stats['total_outages'] > 0:
        print(f"\n--- Outage Statistics ---")
        print(f"Number of Outages: {stats['total_outages']}")
        print(f"Total Downtime: {format_duration(stats['total_downtime_sec'])}")
        print(f"Average Outage Duration: {format_duration(stats['avg_outage_sec'])}")
        print(f"Longest Outage: {format_duration(stats['longest_outage_sec'])}")
        print(f"Shortest Outage: {format_duration(stats['shortest_outage_sec'])}")
    else:
        print(f"\n--- Outage Statistics ---")
        print("No outages detected during this period!")
    
    if stats['avg_latency_ms'] > 0:
        print(f"\n--- Latency Statistics ---")
        print(f"Average Latency: {stats['avg_latency_ms']:.2f} ms")
        print(f"Minimum Latency: {stats['min_latency_ms']} ms")
        print(f"Maximum Latency: {stats['max_latency_ms']} ms")
    
    print("="*60 + "\n")


def find_latest_log(log_dir):
    """Find the most recent log file in the directory."""
    log_files = list(Path(log_dir).glob('uptime_log_*.csv'))
    if not log_files:
        return None
    
    # Sort by modification time, most recent first
    log_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
    return log_files[0]


def main():
    parser = argparse.ArgumentParser(
        description='Query and summarize Internet Uptime Tracker logs'
    )
    parser.add_argument(
        '--log-dir',
        default='logs',
        help='Directory containing log files (default: logs)'
    )
    parser.add_argument(
        '--log-file',
        help='Specific log file to analyze (optional)'
    )
    parser.add_argument(
        '--date',
        help='Analyze logs for a specific date (YYYY-MM-DD format)'
    )
    parser.add_argument(
        '--list',
        action='store_true',
        help='List all available log files'
    )
    
    args = parser.parse_args()
    
    # List mode
    if args.list:
        log_files = list(Path(args.log_dir).glob('uptime_log_*.csv'))
        if not log_files:
            print(f"No log files found in {args.log_dir}")
            return 1
        
        print(f"\nAvailable log files in {args.log_dir}:")
        for log_file in sorted(log_files):
            print(f"  {log_file.name}")
        print()
        return 0
    
    # Determine which log file to analyze
    if args.log_file:
        log_file = Path(args.log_file)
    elif args.date:
        log_file = Path(args.log_dir) / f'uptime_log_{args.date}.csv'
    else:
        log_file = find_latest_log(args.log_dir)
    
    if not log_file:
        print(f"No log files found in {args.log_dir}", file=sys.stderr)
        print("Make sure the service is running and has created log files.", file=sys.stderr)
        return 1
    
    if not log_file.exists():
        print(f"Log file not found: {log_file}", file=sys.stderr)
        return 1
    
    print(f"Analyzing log file: {log_file}")
    
    # Parse and analyze the log
    entries = parse_log_file(log_file)
    if entries is None:
        return 1
    
    if not entries:
        print("No entries found in log file.")
        return 0
    
    stats = calculate_statistics(entries)
    print_statistics(stats)
    
    return 0


if __name__ == '__main__':
    sys.exit(main())
